# LLMLogicEvaluator: Evaluating LLM Performance on Propositional Logic

## Overview

This application evaluates the performance of Large Language Models (LLMs) on propositional logic problems. It uses the `aipip` library to interact with various AI providers and orchestrates a workflow involving problem generation, LLM querying, and results analysis.

The goal is to assess how well different LLMs and configurations can determine the satisfiability of propositional logic formulas.

## Attribution and License

This application is heavily based on the concepts and code structure from the `llmlog` repository by Tanel Tammet:

*   **Original Repository:** [https://github.com/tammet/llmlog](https://github.com/tammet/llmlog)
*   **Original Author:** Tanel Tammet
*   **Original License:** Apache-2.0 License

The refactored code within this application aims to maintain compliance with the original Apache-2.0 license. Please include the original copyright notice and license file if distributing or modifying this derived work significantly.

## Workflow

The evaluation process involves three main stages:

1.  **Problem Generation (`makeproblems` component):
    *   Generates a dataset of propositional logic problems.
    *   Problems vary based on parameters like the number of variables, maximum clause length, and whether they are restricted to Horn clauses.
    *   Generates a balanced mix of satisfiable and unsatisfiable problems.
    *   Outputs problems to a file (e.g., `problems.jsonl`) in JSON Lines format. Each line contains a JSON list with problem metadata, the problem definition (list of clauses), and the ground truth (satisfiability status plus proof/valuation).

2.  **LLM Querying (`askllm` component):
    *   Reads problems from the generated file.
    *   For each problem:
        *   Formats a suitable prompt, potentially including instructions, examples, and the problem represented textually.
        *   Uses the `aipip` library (`TextGenerationService`) to send the prompt to one or more specified LLMs (providers/models) with desired parameters (temperature, max_tokens, etc.).
        *   Receives the textual response from the LLM.
        *   Parses the response to extract the LLM's claim (e.g., "satisfiable" -> 1, "contradiction" -> 0).
    *   Saves the results (including original problem data, LLM claim, full text response, metadata like tokens used if available) to an output file (e.g., `results.jsonl`).

3.  **Results Analysis (`analyze` component):
    *   Reads the results file generated by the querying stage.
    *   Compares the LLM's claim for each problem against the ground truth.
    *   Calculates overall accuracy and potentially breaks down performance by provider, model, and problem characteristics (variable count, clause length, Horn property).
    *   Generates summary statistics and potentially plots/visualizations.

## Core Components (Refactoring Plan)

This application will be structured around refactored versions of the original scripts:

*   **Problem Generator:** Logic adapted from `makeproblems.py`.
*   **LLM Query Runner:** Logic adapted from `askllm.py`, modified to use `aipip.TextGenerationService` for interacting with LLMs.
*   **Results Analyzer:** Logic adapted from `analyze.py`.

## Usage (Placeholder)

```bash
# Example (Conceptual - commands to be defined during implementation)

# 1. Generate problems
# python -m llmlogic_evaluator.generate_problems --output problems.jsonl --count 100 --max-vars 5 ...

# 2. Run LLM queries using aipip
# python -m llmlogic_evaluator.run_queries --input problems.jsonl --output results.jsonl --provider openai --model gpt-4o --provider google --model gemini-1.5-flash ...

# 3. Analyze results
# python -m llmlogic_evaluator.analyze_results --input results.jsonl --output-report report.txt ...
```

Detailed usage instructions will be added as components are implemented.

## Integration with `aipip`

This application relies heavily on the `aipip` library for:

*   Loading API key configuration (`aipip.config`).
*   Instantiating provider clients via the registry (`aipip.providers.registry`).
*   Generating text completions using a unified interface (`aipip.services.TextGenerationService`). 