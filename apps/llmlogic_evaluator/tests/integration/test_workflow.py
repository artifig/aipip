import pytest
import json
import sys
from unittest.mock import MagicMock, patch

# Import components to be tested/mocked
from apps.llmlogic_evaluator.generation import run_generation
from apps.llmlogic_evaluator.querying import run_querying, format_prompt_v1
from apps.llmlogic_evaluator.analysis import run_analysis
from aipip.services.text_generation_service import TextGenerationService
from aipip.providers.interfaces.text_provider import CompletionResponse

# ==========================================
# Test Fixtures
# ==========================================

# Define minimal generation params for predictable output
TEST_GEN_PARAMS = {
    "count": 2, # Generate 1 SAT, 1 UNSAT
    "var_range": [3], # Fixed number of variables
    "len_range": [3], # Fixed clause length
    "horn_flags": [False] # Fixed type (General)
}
TEST_RESOLUTION_STRATEGY = 'original' # Use the faster strategy for tests

# Define mock LLM responses keyed by problem ID and model
MOCK_RESPONSES = {
    # Assume generate creates problem ID 1 (SAT) and 2 (UNSAT) for (3,3,False)
    (1, "model-X"): CompletionResponse(text="This is clearly satisfiable.", provider_name="mock-provider-X", metadata={}),
    (1, "model-Y"): CompletionResponse(text="It looks like a contradiction... wait, no. Satisfiable.", provider_name="mock-provider-Y", metadata={}),
    (2, "model-X"): CompletionResponse(text="I found a contradiction.", provider_name="mock-provider-X", metadata={}),
    (2, "model-Y"): CompletionResponse(text="I'm unsure, but maybe unsatisfiable?", provider_name="mock-provider-Y", metadata={}), # Parsed as UNSAT
}

@pytest.fixture
def mock_aipip_service() -> MagicMock:
    """Provides a mocked TextGenerationService that returns responses sequentially."""
    service = MagicMock(spec=TextGenerationService)

    # Define the expected sequence of responses based on the workflow test
    # Problem 1 (SAT) -> Model X, Model Y
    # Problem 2 (UNSAT) -> Model X, Model Y
    response_sequence = [
        MOCK_RESPONSES[(1, "model-X")], # P1, Model X
        MOCK_RESPONSES[(1, "model-Y")], # P1, Model Y
        MOCK_RESPONSES[(2, "model-X")], # P2, Model X
        MOCK_RESPONSES[(2, "model-Y")], # P2, Model Y
    ]
    response_iterator = iter(response_sequence)

    def generate_side_effect(*args, **kwargs):
        # Simply return the next response in the predefined sequence
        try:
            response = next(response_iterator)
            print(f"Mock service called ({service.generate.call_count}): Returning response for {response.provider_name}:{kwargs.get('model')}")
            return response
        except StopIteration:
            pytest.fail("Mock service generate called more times than expected responses.")
            return None # Should not be reached
        except Exception as e:
            pytest.fail(f"Error in mock service side_effect: {e}")
            return None # Should not be reached

    service.generate.side_effect = generate_side_effect
    return service

@pytest.fixture
def generated_problems_file(tmp_path) -> str:
    """Runs generation to create a predictable input file for query/analyze."""
    gen_output_dir = tmp_path / "generation"
    gen_output_dir.mkdir()
    problems_file = gen_output_dir / "test_problems.jsonl"

    # Use fixed generation params
    run_generation(
        output_file=str(problems_file),
        resolution_strategy=TEST_RESOLUTION_STRATEGY,
        **TEST_GEN_PARAMS
    )
    assert problems_file.exists()
    # Read generated problems to ensure mock responses can be mapped
    # (This is complex, maybe simplify the test setup later)
    problems = []
    with open(problems_file, 'r') as f:
         for line in f:
             problems.append(json.loads(line))
    # TODO: Modify MOCK_RESPONSES or generate_side_effect based on actual IDs/clauses generated
    # For now, assume IDs 1 and 2 correspond to the generated problems.
    print(f"\nGenerated test problems ({len(problems)}): {problems}") # Print for debugging

    return str(problems_file)

# ==========================================
# Integration Test Case(s)
# ==========================================

def test_full_workflow(tmp_path, mock_aipip_service, generated_problems_file):
    """Tests the generate -> query -> analyze workflow integration."""
    # Arrange paths
    problems_file = generated_problems_file # From fixture
    results_file = tmp_path / "test_results.jsonl"
    report_file = tmp_path / "test_report.txt"

    # Arrange query params
    models_to_query = [("mock-provider-X", "model-X"), ("mock-provider-Y", "model-Y")]
    query_params = {"max_tokens": 50}

    # --- Act --- 
    # 1. Query (uses problems_file generated by fixture)
    print(f"\nRunning query step... Input: {problems_file}, Output: {results_file}")
    run_querying(
        input_file=problems_file,
        output_file=str(results_file),
        service=mock_aipip_service,
        model_provider_list=models_to_query,
        **query_params
    )

    # 2. Analyze
    print(f"\nRunning analyze step... Input: {results_file}, Output: {report_file}")
    run_analysis(
        input_file=str(results_file),
        report_file=str(report_file)
    )

    # --- Assert --- 
    # Assert Query Step
    assert results_file.exists()
    results_content = results_file.read_text()
    assert len(results_content.strip().split('\n')) == 4 # 2 problems * 2 models
    # Basic checks on service calls (more detailed checks are tricky)
    assert mock_aipip_service.generate.call_count == 4
    # Check one call structure (example)
    first_call_args, first_call_kwargs = mock_aipip_service.generate.call_args_list[0]
    assert first_call_kwargs.get('provider_name') == "mock-provider-X"
    assert first_call_kwargs.get('model') == "model-X"
    assert first_call_kwargs.get('max_tokens') == 50
    assert "Your task is to solve a problem" in first_call_kwargs.get('prompt', '')

    # Assert Analyze Step
    assert report_file.exists()
    report_content = report_file.read_text()
    print(f"\nGenerated Report:\n{report_content}") # Print report for debugging

    # Verify key metrics based on MOCK_RESPONSES and parsing logic
    # Problem 1 (SAT): model-X -> SAT (Correct), model-Y -> SAT (Correct)
    # Problem 2 (UNSAT): model-X -> UNSAT (Correct), model-Y -> SAT (Incorrect, based on parsing)
    # Overall: Total=4, Correct=3, Unknown=0 => Accuracy = 75.00%
    # Model-X: Total=2, Correct=2, Unknown=0 => Accuracy = 100.00%
    # Model-Y: Total=2, Correct=1, Unknown=0 => Accuracy = 50.00%
    assert "Overall Summary" in report_content
    assert "Total Problems Evaluated: 4" in report_content
    assert "Correct Claims: 3" in report_content
    assert "Unknown/Unparsed Claims: 0" in report_content
    assert "Accuracy (Correct / (Total - Unknown)): 75.00%" in report_content

    assert "Accuracy per Model" in report_content
    assert "Model: model-X" in report_content
    assert "Accuracy: 100.00%" in report_content
    assert "Model: model-Y" in report_content
    assert "Accuracy: 50.00%" in report_content

    assert "Detailed Accuracy per Model and Problem Type" in report_content
    # Check detailed line for Type (3, 3, False)
    assert "Problem Type: Vars=3, Len=3, Type=General" in report_content
    model_x_detail = f"    - {'model-X':<25}: Acc={100.00:>6.2f}% (Correct: {2:>3}/{2-0:>3}, Unknown: {0:>3}, Total: {2:>4})"
    model_y_detail = f"    - {'model-Y':<25}: Acc={50.00:>6.2f}% (Correct: {1:>3}/{2-0:>3}, Unknown: {0:>3}, Total: {2:>4})"
    assert model_x_detail in report_content
    assert model_y_detail in report_content 